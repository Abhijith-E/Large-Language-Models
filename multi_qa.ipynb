{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ae7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import whisper\n",
    "import pyttsx3\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8be58656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa65ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Record voice\n",
    "def record_audio(filename=\"question.wav\", duration=5, fs=44100):\n",
    "    print(\"üé§ Recording your question (Speak now)...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()\n",
    "    write(filename, fs, recording)\n",
    "    print(\"‚úÖ Recording saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6afa4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Transcribe using Whisper\n",
    "def get_voice_input_whisper():\n",
    "    record_audio()\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(\"question.wav\")\n",
    "    question_text = result[\"text\"]\n",
    "    print(\"üó£ You asked:\", question_text)\n",
    "    return question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16ebb252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Detect language\n",
    "def detect_language(text):\n",
    "    lang = detect(text)\n",
    "    if lang.startswith(\"ml\"):  # langdetect may return 'ml'\n",
    "        return \"ml\"\n",
    "    elif lang.startswith(\"fr\"):\n",
    "        return \"fr\"\n",
    "    else:\n",
    "        return \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65c069fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: QA using BERT (English only for now)\n",
    "def answer_with_bert(context, question):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        answer_start = torch.argmax(outputs.start_logits)\n",
    "        answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a556ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Speak answer using TTS\n",
    "def speak_text(text, lang_code=\"en\"):\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    voices = engine.getProperty('voices')\n",
    "    lang_map = {\n",
    "        'en': 'english',\n",
    "        'fr': 'french',\n",
    "        'ml': 'malayalam'\n",
    "    }\n",
    "\n",
    "    for voice in voices:\n",
    "        if lang_map[lang_code].lower() in voice.name.lower():\n",
    "            engine.setProperty('voice', voice.id)\n",
    "            break\n",
    "\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "    engine.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3337d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 7: Multilingual answer handler\n",
    "def main():\n",
    "    # Load PDF (in any language)\n",
    "    context = extract_text_from_pdf(\"english_doc.pdf\")\n",
    "\n",
    "    # Voice input\n",
    "    question = get_voice_input_whisper()\n",
    "    lang = detect_language(question)\n",
    "    print(f\"üåê Detected language: {lang}\")\n",
    "\n",
    "    # Currently, BERT works best with English\n",
    "    if lang != \"en\":\n",
    "        print(\"‚ö†Ô∏è Warning: QA will still run in English model (for demo). Multilingual BERT can be added later.\")\n",
    "\n",
    "    # QA\n",
    "    answer = answer_with_bert(context, question)\n",
    "    print(\"‚úÖ Answer:\", answer)\n",
    "\n",
    "    # Speak answer\n",
    "    speak_text(answer, lang)\n",
    "\n",
    "    os._exit(0)  # Forcefully terminate background threads if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd368a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Recording your question (Speak now)...\n",
      "‚úÖ Recording saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhijith/Christ/LLM/venv310/lib/python3.10/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£ You asked:  –ö—É—á–∞ –±–∞—Ç–∞–ª–∞.\n",
      "üåê Detected language: en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Answer: , including machine learning and deep learning , which enable systems to learn from data and improve their performance over time . ai is revolutionizing diverse sectors , from healthcare where it assists in diagnoses and drug discovery , to Ô¨Ånance where it helps with fraud\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
